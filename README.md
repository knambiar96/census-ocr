# census-ocr
## Overview
This project was a capstone at General Assembly. The purpose was to look into 1950s census data, and hopefully be able to read and process the data of various columns in census data and convert it into a table. While this is the end goal, the capstone focused on just being able to read columns fully- in particular, Name, Race, Gender, Age, Position in Family, Work, Work Location. To start with, a random assortment of census files from 1950 were downloaded. Afterwards, using contour mask, it was attempted to decrease page size to just the page instead of words around it. Afterwards, all header files for each state were carefully clipped, then used to clip the pages down to just the tables- this was done using correlation between the header file and where it should line up on the header and using that to clip down. Finally, a dictionary and measurements were used to reduce that down furthermore down to each individual cell in a table. After labeling all of these cells (and some pages on their own with just the table), the images were put into a padded tensor, along with the labels. The used image tensor is called 'imgtensor_sh.pkl' and the padded labelfile is 'labeltensor_shpad.pkl'. None of the other pkls are used, except in some cases- imgtensor_trsh.pkl - a tensor with the images transposed 90 degrees.

The tensors were modeled using tensorflow-keras with various models- all of which are saved under the models folder in convenient zip files. While all of the models have their charms, the best one so far is the ocr1-324.zip with a Levenshtein Distance of 6.5 on the test set. While this is still insufficient for actual usage in the model, it's a proof of the general idea and a starting point for improving the model in a more fundamental sense. The general model flow is finding a feature map using convolutional layers and GeneralPooling, and then passing it into some preliminary dense layers, then a set of bidirectional LSTMs to better understand sequence data.

## Data Structure
All the Jupyter notebooks are labeled in order, starting from p1 to p-10.1, and are descriptive as to their function. The folder called 'cell-labels' is simply the pre-labeled image files, while 'labeledcells' contains the labeled image files. Sample-Data contains some looks at the data structure without pushing the entire 22 GB stack of images to github. The filelist will further show this data, for those curious.  The JSON folders contain the JSON-labels - in particular, the only one needed is full-annotations.json. The Google Colab Folder is for notebooks that were pushed to Google Colab to be run, and had extra lines added relative to the written code. Finally, the pkl folder contains pkls of all files. Of note, the img_tensor pkls have not been stored because of space- but they are easily reproducible. Furhtermore, the useful label_tensor is labeltensor_shpad.pkl. Finally, the data folder, which while not pushed- it will be important to understand the structure of data and segdata. Data has all 50 states and territories, 2 folders within that each represent some folder from that state and a header file to sue for clipping. Segdata has each state folder, a sub-folder called 'blocks' and files that have been segmented, and labeled if they have been properly segmented or not.



